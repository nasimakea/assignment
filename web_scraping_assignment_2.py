# -*- coding: utf-8 -*-
"""web scraping assignment 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gBI06AKLntfVdeadnKWRUfEphfuQrxvB
"""





"""## Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."""



"""## ANS:-
##
##Web scraping is the process of extracting information or data from websites by using automated techniques or tools. In other words, it involves fetching and parsing the HTML of a web page to extract useful information, such as text, images, links, or structured data. Web scraping is often used when the data needed is not available through an API or when manual data collection is impractical due to the large volume of data or frequent updates.

##Why web scraping is used:

##Data Collection and Analysis: Web scraping is employed to collect large amounts of data from websites for analysis. This could include information related to market trends, pricing, reviews, and other relevant data for business intelligence or research purposes.

##Competitive Intelligence: Companies and businesses use web scraping to gather information about their competitors. This can include monitoring competitor prices, product details, customer reviews, and other data to gain a competitive edge in the market.
"""



"""## Q2. What are the different methods used for Web Scraping?"""



"""## ANS:-
## Web scraping is an automated method of obtaining large amounts of data from websites. Most of this data is unstructured data in HTML format, which is then converted into structured data in a spreadsheet or database so that it can be used in various applications. there are many ways to perform web scraping to get data from websites.

##These include using online services, special APIs, or even creating code for web scraping from scratch. Many large websites, such as Google, Twitter, Facebook, StackOverflow, etc. have APIs that allow you to access your data in a structured format.
"""



"""## Q3. What is Beautiful Soup? Why is it used?"""



"""## ANS:-


##Beautiful Soup is a Python library designed for pulling data out of HTML and XML files. It provides tools for web scraping by allowing developers to search, navigate, and manipulate the parse tree (the hierarchical structure representing the document) of HTML and XML documents. Beautiful Soup transforms a complex HTML or XML document into a tree of Python objects, simplifying the process of extracting information from web pages.
"""



"""## Q4. Why is flask used in this Web Scraping project?"""



"""## Flask is a lightweight and easy-to-use web framework that provides the necessary tools to set up a web server. In a web scraping project, Flask can be used to create a simple web application that serves as the interface for users to initiate and interact with the scraping process."""



"""## Q5. Write the names of AWS services used in this project. Also, explain the use of each service."""



"""## ANS:-

'''
In a web scraping project hosted on AWS (Amazon Web Services), various services may be employed depending on the specific requirements. Here are a few AWS services that could be used:

Amazon EC2 (Elastic Compute Cloud): EC2 provides scalable virtual servers in the cloud. It can be used to host the web scraping application, where Flask and the web scraping logic can run on EC2 instances.

Amazon S3 (Simple Storage Service): S3 is an object storage service used to store and retrieve large amounts of data. In a web scraping project, S3 can be utilized to store the scraped data, acting as a scalable and durable storage solution.

Amazon RDS (Relational Database Service): RDS offers managed relational database services. If the web scraping project involves storing structured data in a database, RDS can be employed to host a relational database (e.g., MySQL, PostgreSQL) to store and manage the data efficiently.

Amazon SQS (Simple Queue Service): SQS is a fully managed message queuing service. It can be used to decouple different components of the web scraping project, especially if there's a need for asynchronous processing or handling a high volume of requests. '''
"""

